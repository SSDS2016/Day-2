{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Data Science Summer School - Split '16 </center>\n",
    "(c) 2016 Martin Tutek\n",
    "\n",
    "version: 0.1\n",
    "\n",
    "kernel: Python 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Decision trees</center>\n",
    "<img style=\"float: center;\" src=\"img/harolds-planet-dtree.jpg\", width=\"40%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python imports & notebook setup\n",
    "# all plots will appear in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set() # make all the plots prettier\n",
    "from ipywidgets import interact, interactive, fixed # more plotting magic\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight') # prettier-plots++\n",
    "\n",
    "# sklearn models & dataset wrangling\n",
    "from sklearn import tree \n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_moons # toy dataset generator\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# make everything reproducible\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "### Reminder: what is a classification problem?\n",
    "\n",
    "We have $M$ $N$-dimensional input samples and $M$ corresponding class values.\n",
    "\n",
    "**Input space:** $\\mathcal{X} \\subset \\mathcal{R}^n$\n",
    "**Output space:** $\\mathcal{Y} = \\{0, \\dots, K\\}$\n",
    "\n",
    "\\begin{array}{lllll|l}\n",
    "&x_1 & x_2 & \\cdots & x_n & \\mathbf{y}\\\\\n",
    "\\hline\n",
    "\\mathbf{x}^{(1)} = & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} & y^{(1)}\\\\\n",
    "\\mathbf{x}^{(2)} = & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} & y^{(2)}\\\\\n",
    "& \\vdots\\\\\n",
    "\\mathbf{x}^{(M)} = & x_1^{(M)} & x_2^{(M)} & \\cdots & x_n^{(M)} & y^{(M)}\\\\\n",
    "\\end{array}\n",
    "\n",
    "**Our goal:** learn a function $f(X) \\to \\mathcal{Y} $.\n",
    "\n",
    "**Dimensions:** input matrix $X_{NxM}$ and output vector $y_M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate our ~~toy~~ difficult dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generating data\n",
    "n_samples = 200 # number of points\n",
    "noise = .05 # gaussian noise\n",
    "moon_xs, moon_ys = make_moons(n_samples=n_samples, noise=noise) # moon graffiti problem\n",
    "single_moon = moon_xs[np.where(moon_ys == 0)]  # take just one moon graffiti for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get some basic information about the dataset\n",
    "print moon_xs.shape\n",
    "print single_moon.shape\n",
    "\n",
    "num_classes = moon_xs.shape[1]\n",
    "print num_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the low-dimensional data\n",
    "for yi in range(num_classes):\n",
    "    plt.scatter(moon_xs[moon_ys == yi, 0], moon_xs[moon_ys == yi, 1], \n",
    "                c = plt.cm.PuOr(yi * 256), label = 'Class {}'.format(yi))\n",
    "plt.title(\"Classification\")\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def very_complicated_classifier(data):\n",
    "    predictions = np.zeros(data.shape[0]) # Assuming the first dimension of our data is M\n",
    "    \n",
    "    ###############################\n",
    "    #        YOUR CODE HERE       #\n",
    "    ###############################\n",
    "    \n",
    "    # Create a classifier that makes a decision based on values from a single dimension\n",
    "    # of the input data. We are working in continuous space, so use > and < to separate\n",
    "    # based on some value.\n",
    "    \n",
    "    # What are the dimensions of our problem? \n",
    "    # Where could we split the data for a best classification?\n",
    "\n",
    "    ###############################\n",
    "    #       END OF YOUR CODE      #\n",
    "    ###############################\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate decision boundary plot data\n",
    "plot_step = .01 #interval of x and y value steps\n",
    "\n",
    "x_min, x_max = moon_xs[:, 0].min() - 1, moon_xs[:, 0].max() + 1 # x values range\n",
    "y_min, y_max = moon_xs[:, 1].min() - 1, moon_xs[:, 1].max() + 1 # y values range\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "                np.arange(x_min, x_max, plot_step),\n",
    "                np.arange(y_min, y_max, plot_step)\n",
    "                    )\n",
    "\n",
    "vcc_predictions = very_complicated_classifier(np.c_[xx.ravel(), yy.ravel()])\n",
    "vcc_predictions = vcc_predictions.reshape(xx.shape)\n",
    "\n",
    "plt.scatter(moon_xs[:, 0], moon_xs[:, 1], c=moon_ys, cmap = plt.cm.PuOr)\n",
    "cs = plt.contourf(xx, yy, vcc_predictions, cmap = plt.cm.PuOr, alpha=0.5)\n",
    "plt.title('Very complicated classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=1).fit(moon_xs, moon_ys)\n",
    "\n",
    "dt_prediction = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "dt_prediction = dt_prediction.reshape(xx.shape)\n",
    "\n",
    "# Setup the parallel plots\n",
    "f, axarr = plt.subplots(1, 2, figsize=(9, 4.5))\n",
    "\n",
    "# Plot the points\n",
    "axarr[0].scatter(moon_xs[:, 0], moon_xs[:, 1], c=moon_ys, cmap = plt.cm.PuOr)\n",
    "# Plot the decision boundary\n",
    "cs = axarr[0].contourf(xx, yy, vcc_predictions, cmap = plt.cm.PuOr, alpha=0.5)\n",
    "\n",
    "axarr[0].set_title('Very complicated classifier')\n",
    "\n",
    "\n",
    "axarr[1].scatter(moon_xs[:, 0], moon_xs[:, 1], c = moon_ys, cmap = plt.cm.PuOr)\n",
    "cs = axarr[1].contourf(xx, yy, dt_prediction, cmap = plt.cm.PuOr, alpha=0.5)\n",
    "axarr[1].set_title('Decision tree classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the decision tree classifier\n",
    "\n",
    "> class sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, class_weight=None, presort=False)\n",
    "\n",
    "Great documentation at: **[Click me!](http://scikit-learn.org/stable/modules/tree.html)**\n",
    "\n",
    "### Which values do we actually pay attention to? (for now)\n",
    "\n",
    "* **criterion** \n",
    "  * The criterion is the method we use to decide on which node to split the tree. In the previous example, we arbitrarily chose a point for splitting, however machines don't have free will. Therefore we define mathematical ways of choosing the best point for splits\n",
    "  \n",
    "* **max_depth**  \n",
    "  * The amount of splits done while growing (training) the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the tree\n",
    "\n",
    "Decision trees are famed due to their **interpretability** and **simplicity**. Even though models based on decision trees which are used in practice are not as interpretable due to using many trees at the same time, it is usually good practice to be able to visualise the result of the $n$ most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "import pydotplus\n",
    "\n",
    "def model_to_plot(classifier, outfile, feature_names=['x', 'y'], class_names=['0', '1']):\n",
    "    dot_data = StringIO() \n",
    "    tree.export_graphviz(classifier, out_file=dot_data,  \n",
    "                             feature_names=feature_names,  \n",
    "                             class_names=class_names,  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True)  \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "    graph.write_pdf(outfile) \n",
    "\n",
    "outfile = \"out/moons.pdf\"\n",
    "model_to_plot(clf, outfile)\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"out/moons.pdf\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        depth = i * 3 + j + 1 # complicated magic\n",
    "        clf = tree.DecisionTreeClassifier(max_depth=depth).fit(moon_xs, moon_ys)\n",
    "\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axarr[i,j].scatter(moon_xs[:, 0], moon_xs[:, 1], c = moon_ys, cmap = plt.cm.PuOr)\n",
    "        cs = axarr[i,j].contourf(xx, yy, Z, cmap = plt.cm.PuOr, alpha=0.5)\n",
    "        \n",
    "        axarr[i,j].set_title(\"Depth = {}\".format(depth), fontsize=12)\n",
    "        axarr[i,j].axes.get_xaxis().set_ticks([])\n",
    "        axarr[i,j].axes.get_yaxis().set_ticks([])\n",
    "\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interact demo (optional)\n",
    "@interact(max_depth=(1,10))\n",
    "def classify(max_depth):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=max_depth).fit(moon_xs, moon_ys)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.scatter(moon_xs[:, 0], moon_xs[:, 1], c = moon_ys, cmap = plt.cm.PuOr)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap = plt.cm.PuOr, alpha=0.5)\n",
    "    plt.title('Classifier results')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Regression is in its heart the **same** thing as classification, except you can imagine each point as having its own class. The only thing that changes is the definition of the output space - it is no longer a discrete set of possible values, but a (real-valued?) number with possibly infinitely large cardinality.\n",
    "\n",
    "**Input space:** $\\mathcal{X} \\subset \\mathcal{R}^n$\n",
    "**Output space:** $\\mathcal{Y} \\subset \\mathcal{R}$\n",
    "\n",
    "**Our goal:** learn a function $f(X) \\to \\mathcal{Y} $.\n",
    "\n",
    "**Dimensions:** input matrix $X_{NxM}$ and output vector $y_M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.scatter(single_moon[:, 0], single_moon[:, 1], c=np.zeros(single_moon.shape[0]), cmap = plt.cm.PuOr)\n",
    "plt.title('Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to solve this new, completely different problem?\n",
    "   \n",
    "Use [`DecisionTreeRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) instead of [`DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). \n",
    "\n",
    "Everything else is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, one of our input dimensions (y) becomes the value which we are trying to predict.\n",
    "X = single_moon[:, 0]\n",
    "y = single_moon[:, 1]\n",
    "\n",
    "# Generate the test data (sample decision points for the whole space of X)\n",
    "plot_step = .01 \n",
    "x_min, x_max = X.min() - .1, X.max() + .1\n",
    "X_test = np.arange(x_min, x_max, plot_step)\n",
    "\n",
    "# The classifier expects 2d data!\n",
    "X = X.reshape(X.shape[0], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1)\n",
    "\n",
    "# Plot X, y\n",
    "plt.scatter(X, y, c = 'r')\n",
    "\n",
    "# Loops are better than copy/pasting\n",
    "depths = [2, 9] # Regression tree depth\n",
    "colors = [\"g\",\"b\"] # Line color\n",
    "linewidths = [3, 1] # Line width\n",
    "\n",
    "# Iterate, train, predict and plot\n",
    "for depth, color, linewidth in zip(depths, colors, linewidths):\n",
    "    # The magical three lines\n",
    "    reg = tree.DecisionTreeRegressor(max_depth=depth)\n",
    "    reg.fit(X,y)\n",
    "    predictions = reg.predict(X_test)\n",
    "    \n",
    "    plt.plot(X_test, predictions, c=color, label=\"max_depth={}\".format(depth), linewidth=linewidth, alpha = 0.5)\n",
    "    \n",
    "plt.legend(fontsize='10')\n",
    "plt.title('Regression vs tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of depth 2: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a 3x3 grid of plots\n",
    "f, axarr = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        depth = (i * 3) + (j + 1) # complicated magic\n",
    "        clf = tree.DecisionTreeRegressor(max_depth=depth).fit(X, y)\n",
    "\n",
    "        preds = clf.predict(X_test)\n",
    "        \n",
    "        axarr[i,j].scatter(X, y, c = 'r')\n",
    "        axarr[i,j].plot(X_test, preds, c=\"g\", label=\"max_depth={}\".format(depth), linewidth=1., alpha=0.7)\n",
    "        axarr[i,j].set_title(\"Depth = {}\".format(depth), fontsize=12)\n",
    "\n",
    "        axarr[i,j].axes.get_xaxis().set_ticks([])\n",
    "        axarr[i,j].axes.get_yaxis().set_ticks([])\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision tree learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criterion: Information gain (Kullbach - Liebler divergence)\n",
    "**Formal definition:** The expected information gain (IG) is the change in information entropy H from a prior state to a state that takes some information. \n",
    "\n",
    "$$ IG(D_p, f) = I(D_p) - \\sum_{j=1}^{m} \\frac{N_j}{N} I(D_j) $$\n",
    "\n",
    "Where $D_p$ is the **parent** dataset at the point where we decide the split, $f = (f_1, \\dots, f_j, \\dots, f_m)$ the feature for which we are calculating the information gain, $D_j$ the dataset left after splitting the parent dataset on each feature value $f_j$ and $N_j$ is the number of samples in the part of dataset $D_j$.\n",
    "\n",
    "Here, $I$ represents the generalized **measure of impurity** of the data. Entropy is one instance of an impurity measure, alongside which Gini impurity and classification error are also used. Information gain criterion chooses the split that **maximizes** the purity of the data, which can be seen as the **homogeneity** of the data post-split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impurity measures for classification\n",
    "#### Entropy\n",
    "Shannon's entropy (H) is defined in information theory as the expected value of information contained in each message. In simpler terms, it captures the **uncertainty** of a random variable.\n",
    "\n",
    "$$ H(p) \\stackrel{binary}{=} -p \\cdot log_2(p) - (1 - p) \\cdot log_2(1-p) \\stackrel{general}{=} -\\sum_{i=1}^{C} p_i \\cdot {log_2 p_i} $$\n",
    "\n",
    "\n",
    "Where $p_i$ is the probability of outcome $i$ from the set of all possible outcomes $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some class probabilities to visualize various impurity measures\n",
    "step_size = .01\n",
    "X = np.arange(0, 1 + step_size, step_size)\n",
    "\n",
    "X = np.vstack((X, 1. - X))\n",
    "assert all((X[0] + X[1]) == 1), \"Probabilities don't sum to one\"\n",
    "# We get an array of (p, 1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Entropy - \n",
    "def unsafe_entropy(probabilities):\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "def entropy(probabilities): \n",
    "    nonzero_indices = np.where(probabilities > 0.)\n",
    "    probabilities = probabilities[nonzero_indices]\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "print entropy(np.array([.5, .5])), unsafe_entropy(np.array([.5, .5]))\n",
    "print entropy(np.array([1.])), unsafe_entropy(np.array([1.]))\n",
    "print entropy(np.array([0., 1.])), unsafe_entropy(np.array([0., 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hx = np.apply_along_axis(entropy, 0, X) # Calculate entropy for each (p, 1-p) pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini impurity\n",
    "\n",
    "**Formally:** Gini impurity (G) is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n",
    "\n",
    "$$ G(p) \\stackrel{general}{=} \\sum_{i=1}^{C} p_i \\cdot (1 - p_i) = \\sum_{i=1}^{C} p_i - \\sum_{i=1}^{C} p_i^2 = 1 - \\sum_{i=1}^{C} p_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gini impurity\n",
    "def gini(probabilities):\n",
    "    return 1. - np.sum(probabilities * probabilities)\n",
    "\n",
    "gx = np.apply_along_axis(gini, 0, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification error\n",
    "The classification error (CE) is defined as the uncertainty of all but the most probable variable. \n",
    "\n",
    "$$ CE = 1 - \\max_i p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classification_error(probabilites):\n",
    "    return 1. - np.max(probabilites)\n",
    "\n",
    "cex = np.apply_along_axis(classification_error, 0, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pretty colors\n",
    "colors = sns.color_palette(\"muted\")\n",
    "\n",
    "plt.plot(X[0], hx, label='Entropy', c = colors[0])\n",
    "plt.plot(X[0], hx/2, label='Scaled entropy', c = colors[0], alpha=0.75)\n",
    "plt.plot(X[0], gx, label='Gini', c = colors[1], alpha = 0.5)\n",
    "plt.plot(X[0], cex, label='Classification error', c = colors[2], alpha = 0.5)\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.xlabel('$p$', fontsize=15)\n",
    "plt.ylabel('$Impurity$', fontsize=15)\n",
    "plt.title(\"Impurity measures for classification\", fontsize=15)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity measure~~(s)~~ for regression\n",
    "\n",
    "### Mean squared error\n",
    "\n",
    "We still use the same criterion, however the impurity measure is the mean square error (MSE) of a split at a certain value of a feature. \n",
    "\n",
    "To fully understand this, you have to know that the prediction for the output value at a certain node is the mean target value of all the training samples that have reached that node. Therefore, we can define the MSE of a node as:\n",
    "\n",
    "$$ MSE(D) = \\frac{1}{N_d} \\sum_{i}^{N_d} (\\hat{Y} - Y_i)^2$$\n",
    "\n",
    "Where $D$ is the subset of the dataset at the current node, $N_d$ the amount of samples in the subset of the dataset, and $\\hat{Y}$ the prediction - or the mean target value of all the samples in $D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tennis dataset\n",
    "Meet John. John is very lazy. John is actually so lazy that he does not want to make his own decisions for himself, but would rather automate the process. In order to do so, he tracked weather data at times that his friends invited him to play tennis and the outcomes of those invites. He wants to not only train a classifier that makes decisions for him, but also understand how it works.\n",
    "\n",
    "\n",
    "#### Working with categorical and ordinal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/tennis-categorical.csv\")\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split columns into X and y names\n",
    "features = df.columns.values[:-1]\n",
    "target = df.columns.values[-1]\n",
    "target_values = list(np.unique(df[target]))\n",
    "print features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify important features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"muted\")\n",
    "\n",
    "f, axarr = plt.subplots(2, 2, figsize=(6, 6), sharey=True)\n",
    "f.suptitle('Class count / entropy per feature value', fontsize=15)\n",
    "\n",
    "measures = {entropy: [], gini:[], classification_error:[]}\n",
    "\n",
    "for idx, feature in enumerate(features): \n",
    "\n",
    "    feature_values = np.unique(df[feature])\n",
    "    \n",
    "    target_stats = {target:[] for target in target_values}\n",
    "    \n",
    "    for feature_value in feature_values: \n",
    "        subset = df.ix[df[feature] == feature_value]\n",
    "        target_distribution = Counter(subset[target])\n",
    "        \n",
    "        for t in target_values: \n",
    "            if t in target_distribution: \n",
    "                target_stats[t].append(target_distribution[t])\n",
    "            else:\n",
    "                target_stats[t].append(0)\n",
    "    \n",
    "    #########################\n",
    "    #       plotting\n",
    "    #########################\n",
    "    \n",
    "    N = len(feature_values)\n",
    "    indices = np.arange(N)\n",
    "    width = 0.15\n",
    "    \n",
    "    i = idx / 2\n",
    "    j = idx % 2\n",
    "    \n",
    "    # This probably could have been done a LOT easier \n",
    "    vals = [target_stats[t] for t in target_stats]\n",
    "    freqs = zip(vals[0], vals[1])\n",
    "    probs = [np.array(list(freq), dtype=float) for freq in freqs]\n",
    "    probs = [prob / np.sum(prob) for prob in probs] \n",
    "    entropies = [entropy(prob) for prob in probs]\n",
    "    \n",
    "    for idx, t in enumerate(target_stats):\n",
    "        wplus = idx * width\n",
    "        #print wplus, target_stats[t]\n",
    "        axarr[i,j].bar(indices + wplus, target_stats[t], width, label=t, color=colors[1 -idx])\n",
    "\n",
    "        axarr[i,j].set_xlabel(feature, fontsize=12)\n",
    "        \n",
    "    wplus = 2 * width\n",
    "    axarr[i,j].bar(indices + wplus, entropies, width, label='Entropy', color='r')\n",
    "\n",
    "               \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sklearn does not work with categorical features!\n",
    "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(df[features], df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's try again\n",
    "dummy_features = pd.get_dummies(df[features])\n",
    "print dummy_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(dummy_features, df[target])\n",
    "feature_names = dummy_features.columns.values\n",
    "\n",
    "model_to_plot(clf, 'out/tennistree.pdf', feature_names, target_values)\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"out/tennistree.pdf\", width=800, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
